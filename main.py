import os
import requests
import json
import re
import xml.etree.ElementTree as ET
from datetime import datetime
from openai import OpenAI

# === 1. é…ç½®åŒºåŸŸ ===
WEBHOOK_URL = os.environ.get("WECOM_WEBHOOK_KEY")
DEEPSEEK_KEY = os.environ.get("DEEPSEEK_API_KEY")

# åˆå§‹åŒ– DeepSeek
client = OpenAI(
    api_key=DEEPSEEK_KEY,
    base_url="https://api.deepseek.com"
)

# === å·¥å…·å‡½æ•°ï¼šé€šç”¨ RSS è§£æå™¨ ===
def fetch_rss_data(source_name, rss_url):
    """
    é€šç”¨çš„ RSS æŠ“å–å‡½æ•°
    """
    print(f"ğŸ”„ [{source_name}] æ­£åœ¨è¿æ¥...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    items = []
    try:
        response = requests.get(rss_url, headers=headers, timeout=15)
        if response.status_code == 200:
            content = response.content
            try:
                root = ET.fromstring(content)
                nodes = root.findall('./channel/item')
                if not nodes:
                    nodes = root.findall('.//{http://purl.org/rss/1.0/}item')
                if not nodes:
                    nodes = root.findall('item')

                for item in nodes[:10]: 
                    title_node = item.find('title')
                    if title_node is None: 
                        title_node = item.find('{http://purl.org/rss/1.0/}title')
                    title = title_node.text if title_node is not None else "æ— æ ‡é¢˜"

                    link_node = item.find('link')
                    if link_node is None:
                        link_node = item.find('{http://purl.org/rss/1.0/}link')
                    link = link_node.text if link_node is not None else ""

                    desc_node = item.find('description')
                    if desc_node is None:
                        desc_node = item.find('{http://purl.org/rss/1.0/}description')
                    desc = desc_node.text if desc_node is not None else ""
                    desc = re.sub(r'<[^>]+>', '', desc) # å»é™¤HTMLæ ‡ç­¾

                    if title and link:
                        items.append({
                            "source": source_name,
                            "title": title,
                            "summary": desc[:200],
                            "url": link
                        })
                print(f"âœ… [{source_name}] è·å–åˆ° {len(items)} æ¡")
            except Exception as e:
                print(f"âŒ [{source_name}] XML è§£æå¤±è´¥: {e}")
        else:
            print(f"âŒ [{source_name}] è¯·æ±‚å¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ [{source_name}] ç½‘ç»œé”™è¯¯: {e}")
    
    return items

def fetch_readhub():
    """
    ReadHub ä¸“ç”¨æŠ“å–
    """
    print(f"ğŸ”„ [ReadHub] æ­£åœ¨è¿æ¥...")
    url = "https://api.readhub.cn/topic?pageSize=10"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Origin": "https://readhub.cn",
        "Referer": "https://readhub.cn/"
    }
    items = []
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            data = resp.json().get('data', [])
            for d in data:
                items.append({
                    "source": "ReadHub",
                    "title": d.get('title'),
                    "summary": d.get('summary', '')[:200],
                    "url": f"https://readhub.cn/topic/{d.get('id')}"
                })
            print(f"âœ… [ReadHub] è·å–åˆ° {len(items)} æ¡")
        else:
            print(f"âŒ [ReadHub] çŠ¶æ€ç : {resp.status_code}")
    except Exception as e:
        print(f"âŒ [ReadHub] å¤±è´¥: {e}")
    return items

def get_all_news():
    """
    èšåˆæ‰€æœ‰æ•°æ®æº
    """
    all_news = []
    all_news.extend(fetch_readhub())
    all_news.extend(fetch_rss_data("36Kr", "https://36kr.com/feed"))
    all_news.extend(fetch_rss_data("Solidot", "https://www.solidot.org/index.rss"))
    all_news.extend(fetch_rss_data("InfoQ", "https://www.infoq.cn/feed"))
    return all_news

def process_news_with_ai(news_list):
    """
    AI ç­›é€‰ä¸ç‚¹è¯„ï¼ˆPrompt å·²æ›´æ–°ï¼šåŒé‡è§’è‰² + çº¯å‡€è¾“å‡ºï¼‰
    """
    if len(news_list) > 45:
        print(f"âœ‚ï¸ æ–°é—»å¤ªå¤š({len(news_list)}æ¡)ï¼Œæˆªå–å‰ 45 æ¡å–‚ç»™ AI...")
        news_list = news_list[:45]

    print(f"ğŸ§  AI (äº§å“ä¸“å®¶ & ä½“éªŒè®¾è®¡å¸ˆ) æ­£åœ¨é˜…è¯» {len(news_list)} æ¡æ–°é—»...")
    
    raw_text = json.dumps(news_list, ensure_ascii=False)
    
    # === æ ¸å¿ƒä¿®æ”¹åŒºåŸŸï¼šäººè®¾ä¸è¦æ±‚ ===
    system_prompt = """
    ä½ æ˜¯ä¸€ä½æ‹¥æœ‰åŒé‡è§†è§’çš„ä¸“å®¶ï¼šæ—¢æ˜¯ã€èµ„æ·±æ•°æ®äº§å“ä¸“å®¶ã€‘ï¼Œåˆæ˜¯ã€æ•°æ®äº§å“ä½“éªŒè®¾è®¡å¸ˆã€‘ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯ä¸ºåŒè¡Œä¸šè€…ç­›é€‰å‡º 4-6 æ¡æœ€æœ‰ä»·å€¼çš„èµ„è®¯ã€‚
    
    ã€ç­›é€‰ä¼˜å…ˆçº§ã€‘ï¼š
    1. æ ¸å¿ƒå…³æ³¨ï¼šAI Agent äº¤äº’èŒƒå¼ã€æ•°æ®å¯è§†åŒ–åˆ›æ–°ã€BI å·¥å…·çš„æ–°ä½“éªŒè®¾è®¡ã€‚
    2. æ ¸å¿ƒå…³æ³¨ï¼šæ•°æ®æ¶æ„å˜é©ã€å¤§æ¨¡å‹è½åœ°ï¼ˆRAG/å‘é‡åº“ï¼‰çš„æŠ€æœ¯çªç ´ã€‚
    3. é‡è¦å…³æ³¨ï¼šä¸»æµç§‘æŠ€å¤§å‚ï¼ˆOpenAI/Google/Figmaç­‰ï¼‰å¯¹æ•°æ®äº§å“çš„è®¾è®¡è°ƒæ•´ã€‚
    
    ã€è¾“å‡ºé“å¾‹ã€‘ï¼š
    1. æ ‡é¢˜ï¼šç®€ç»ƒã€ä¸“ä¸šã€‚
    2. ç‚¹è¯„ï¼ˆCommentï¼‰ï¼š
       - å¿…é¡»ç»“åˆâ€œå•†ä¸šä»·å€¼â€æˆ–â€œç”¨æˆ·ä½“éªŒâ€è¿›è¡Œæ·±åº¦æ´å¯Ÿã€‚
       - **ä¸¥ç¦**å‡ºç°â€œä½œä¸ºè®¾è®¡å¸ˆâ€ã€â€œç¬”è€…è®¤ä¸ºâ€ã€â€œä»äº§å“è§’åº¦çœ‹â€ç­‰èº«ä»½æŒ‡ä»£è¯ã€‚
       - **ä¸¥ç¦**å†™â€œè¿™æ¡æ–°é—»ä»‹ç»äº†...â€è¿™ç±»åºŸè¯ã€‚
       - ç›´æ¥è¾“å‡ºè§‚ç‚¹ã€‚ä¾‹å¦‚ï¼šâ€œæ­¤åŠŸèƒ½å°†å¤§å¹…é™ä½éæŠ€æœ¯äººå‘˜çš„å–æ•°é—¨æ§›ï¼Œæ˜¯æ•°æ®æ°‘ä¸»åŒ–çš„å…³é”®ä¸€æ­¥ã€‚â€

    è¯·è¿”å› JSON æ•°ç»„æ ¼å¼ï¼š
    [
        {
            "title": "é‡å†™åçš„æ ‡é¢˜",
            "source": "æ¥æº",
            "comment": "ç›´æ¥çš„çŠ€åˆ©ç‚¹è¯„",
            "url": "é“¾æ¥"
        }
    ]
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·ç­›é€‰å¹¶åˆ†æï¼š{raw_text}"}
            ],
            response_format={ "type": "json_object" }, 
            temperature=0.3
        )
        content = response.choices[0].message.content
        
        if content.startswith("```"):
            content = re.sub(r"^```json\s*|\s*```$", "", content, flags=re.MULTILINE)
            
        result = json.loads(content)
        
        if isinstance(result, dict):
            for k, v in result.items():
                if isinstance(v, list): return v
        return result if isinstance(result, list) else []
        
    except Exception as e:
        print(f"âŒ AI æ€è€ƒå¤±è´¥: {e}")
        return []

def send_wecom(news_list):
    if not WEBHOOK_URL: return
    if not news_list: return

    today = datetime.now().strftime("%Y-%m-%d")
    content_lines = [f"### ğŸš€ AI æ•°æ®äº§å“æ—¥æŠ¥ ({today})"]
    
    for idx, news in enumerate(news_list, 1):
        title = news.get('title', 'æ— æ ‡é¢˜')
        url = news.get('url', '#')
        comment = news.get('comment', 'æ— ç‚¹è¯„')
        src = news.get('source', 'ç²¾é€‰')
        
        content_lines.append(f"**{idx}. {title}**")
        content_lines.append(f"_{src}_  [æŸ¥çœ‹åŸæ–‡]({url})")
        content_lines.append(f"> ğŸ’¡ {comment}") 
        content_lines.append("")

    data = {"msgtype": "markdown", "markdown": {"content": "\n".join(content_lines)}}
    
    try:
        requests.post(WEBHOOK_URL, json=data)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

if __name__ == "__main__":
    all_data = get_all_news()
    
    if not all_data:
        print("âŒ æ‰€æœ‰æºéƒ½æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚")
    else:
        print(f"ğŸ“¦ æ€»å…±è·å–åˆ° {len(all_data)} æ¡å€™é€‰æ–°é—»")
        final_news = process_news_with_ai(all_data)
        if final_news:
            send_wecom(final_news)
        else:
            print("âš ï¸ AI è§‰å¾—ä»Šå¤©æ²¡ä»€ä¹ˆå€¼å¾—çœ‹çš„æ–°é—»ã€‚")
