import os
import requests
import json
import re
from datetime import datetime
from openai import OpenAI
from bs4 import BeautifulSoup 

# === 1. é…ç½®åŒºåŸŸ ===
# âš ï¸ è°ƒè¯•æ¨¡å¼ï¼šTrue = åªæ‰“å°ä¸å‘é€ï¼›False = æ­£å¼å‘é€
# éªŒè¯é€šè¿‡åï¼Œè®°å¾—æ”¹æˆ False
DRY_RUN = True 

WEBHOOK_URL = os.environ.get("WECOM_WEBHOOK_KEY")
DEEPSEEK_KEY = os.environ.get("DEEPSEEK_API_KEY")

client = OpenAI(
    api_key=DEEPSEEK_KEY,
    base_url="https://api.deepseek.com"
)

# === 2. ç½‘é¡µçˆ¬è™« ===
def fetch_uisdc_news_html():
    target_url = "https://www.uisdc.com/news"
    print(f"ğŸ”„ æ­£åœ¨åƒæµè§ˆå™¨ä¸€æ ·è®¿é—®: {target_url}")
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://www.uisdc.com/",
        "Accept-Language": "zh-CN,zh;q=0.9"
    }
    
    items = []
    try:
        response = requests.get(target_url, headers=headers, timeout=15)
        response.encoding = 'utf-8' 
        
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, 'html.parser')
            # æŸ¥æ‰¾ h3 æˆ– h2 æ ‡ç­¾
            news_nodes = soup.find_all(['h3', 'h2'])
            
            count = 0
            for node in news_nodes:
                if count >= 8: break 
                
                link_tag = node.find('a')
                if not link_tag: continue
                
                title = link_tag.get_text(strip=True)
                href = link_tag.get('href')
                
                if len(title) < 5: continue 
                
                desc = ""
                parent = node.parent
                desc_node = parent.find('p')
                if desc_node:
                    desc = desc_node.get_text(strip=True)
                
                if href and not href.startswith('http'):
                    href = f"https://www.uisdc.com{href}"
                    
                if title and href:
                    items.append({
                        "title": title,
                        "original_summary": desc[:200],
                        "url": href
                    })
                    count += 1
            
            print(f"âœ… æˆåŠŸä»é¡µé¢è§£æå‡º {len(items)} æ¡æ–°é—»")
            return items
        else:
            print(f"âŒ é¡µé¢è¯·æ±‚å¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ çˆ¬è™«å‘ç”Ÿé”™è¯¯: {e}")
    
    return items

# === 3. AI å¤„ç†é€»è¾‘ (å·²ä¿®å¤ Prompt é—®é¢˜) ===
def process_news_with_ai(news_list):
    if not news_list: return []
    
    print(f"ğŸ§  AI æ­£åœ¨é˜…è¯»å¹¶æç‚¼ {len(news_list)} æ¡æ–°é—»...")
    
    input_data = [{"title": n["title"], "summary": n["original_summary"], "url": n["url"]} for n in news_list]
    raw_text = json.dumps(input_data, ensure_ascii=False)
    
    # === å…³é”®ä¿®æ”¹ï¼šPrompt é‡Œæ˜ç¡®åŠ ä¸Šäº† "JSON" è¿™ä¸ªè¯ ===
    system_prompt = """
    ä½ æ˜¯ä¸€ä½ã€æç®€èµ„è®¯ç¼–è¾‘ã€‘ã€‚ä½ çš„ä»»åŠ¡æ˜¯é‡å†™ä¼˜è®¾è¯»æŠ¥çš„æ‘˜è¦ã€‚
    
    ã€å¤„ç†è¦æ±‚ã€‘ï¼š
    1. **æ ‡é¢˜**ï¼šä¼˜åŒ–æ ‡é¢˜ï¼Œä½¿å…¶æ›´å¸å¼•è®¾è®¡å¸ˆã€‚
    2. **æ‘˜è¦**ï¼š
       - **å®Œå…¨é‡å†™**åŸæ–‡ã€‚
       - **æçŸ­**ï¼šæ§åˆ¶åœ¨ 30-40 å­—ä»¥å†…ã€‚
       - **ç›´å‡»é‡ç‚¹**ï¼šç›´æ¥è¯´è¿™ä¸ªå·¥å…·æˆ–æ–°é—»å¯¹è®¾è®¡å¸ˆæœ‰ä»€ä¹ˆç”¨ã€‚
       - **ä¿ç•™ URL**ï¼šå¿…é¡»åŸæ ·è¿”å› URLã€‚

    ã€è¾“å‡ºæ ¼å¼ã€‘ï¼š
    è¯·ä¸¥æ ¼è¿”å› JSON æ ¼å¼æ•°æ® (Return JSON):
    {
        "news": [
            {
                "title": "æ–°æ ‡é¢˜",
                "summary": "æç®€æ‘˜è¦",
                "url": "åŸå§‹URL"
            }
        ]
    }
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·å¤„ç†ï¼š{raw_text}"}
            ],
            response_format={ "type": "json_object" }, 
            temperature=0.3
        )
        content = response.choices[0].message.content
        if content.startswith("```"): content = re.sub(r"^```json\s*|\s*```$", "", content, flags=re.MULTILINE)
        result = json.loads(content)
        
        if isinstance(result, dict):
            if "news" in result: return result["news"]
            for k, v in result.items():
                if isinstance(v, list): return v
        return []
    except Exception as e:
        print(f"âŒ AI å¤„ç†å¤±è´¥: {e}")
        return []

# === 4. æ¨é€é€»è¾‘ ===
def send_wecom(news_list):
    if not news_list: return

    today = datetime.now().strftime("%mæœˆ%dæ—¥")
    content_lines = [f"### ğŸ¨ ä¼˜è®¾çµæ„Ÿæ—©æŠ¥ ({today})"]
    
    for idx, news in enumerate(news_list, 1):
        title = news.get('title', 'æ— æ ‡é¢˜')
        url = news.get('url', '#')
        summary = news.get('summary', 'æš‚æ— ä»‹ç»')
        
        content_lines.append(f"**{idx}. [{title}]({url})**")
        content_lines.append(f"> {summary}")
        content_lines.append("") 

    final_content = "\n".join(content_lines)

    if DRY_RUN:
        print("\n" + "="*30)
        print("ğŸ“¢ [æ¨¡æ‹Ÿå‘é€] æœ€ç»ˆæ•ˆæœå¦‚ä¸‹ï¼š")
        print("="*30)
        print(final_content)
        print("="*30 + "\n")
        return

    if not WEBHOOK_URL: return
    data = {"msgtype": "markdown", "markdown": {"content": final_content}}
    try:
        requests.post(WEBHOOK_URL, json=data)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

if __name__ == "__main__":
    # 1. çˆ¬å– HTML
    raw_news = fetch_uisdc_news_html()
    
    if not raw_news:
        print("âŒ æ²¡æŠ“åˆ°ä»»ä½•æ–°é—»")
    else:
        # 2. AI æ¶¦è‰²
        final_news = process_news_with_ai(raw_news)
        if final_news:
            send_wecom(final_news)
        else:
            print("âš ï¸ AI æœªç­›é€‰å‡ºç»“æœ")
