import os
import requests
import json
import re
import xml.etree.ElementTree as ET
from datetime import datetime
from openai import OpenAI

# === 1. é…ç½®åŒºåŸŸ ===
WEBHOOK_URL = os.environ.get("WECOM_WEBHOOK_KEY")
DEEPSEEK_KEY = os.environ.get("DEEPSEEK_API_KEY")

# åˆå§‹åŒ– DeepSeek
client = OpenAI(
    api_key=DEEPSEEK_KEY,
    base_url="https://api.deepseek.com"
)

# === å·¥å…·å‡½æ•°ï¼šé€šç”¨ RSS è§£æå™¨ (å¢å¼ºç‰ˆ) ===
def fetch_rss_data(source_name, rss_url):
    """
    é€šç”¨çš„ RSS æŠ“å–å‡½æ•°
    """
    print(f"ğŸ”„ [{source_name}] æ­£åœ¨è¿æ¥...")
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    items = []
    try:
        response = requests.get(rss_url, headers=headers, timeout=15)
        if response.status_code == 200:
            # å…¼å®¹å¤„ç†ï¼šæœ‰äº› RSS ç¼–ç å¯èƒ½æ˜¯ä¹±ç ï¼Œä½¿ç”¨ content è®© ET è‡ªåŠ¨å¤„ç†
            try:
                root = ET.fromstring(response.content)
                
                # æš´åŠ›æŸ¥æ‰¾æ‰€æœ‰çš„ item æ ‡ç­¾ï¼ˆä¸ç®¡å®ƒè—åœ¨å¤šæ·±çš„å±‚çº§é‡Œï¼‰
                # è¿™ç§å†™æ³•èƒ½åŒæ—¶æå®š RSS 2.0, RSS 1.0 (Solidot) å’Œéƒ¨åˆ† Atom
                nodes = root.findall('.//item') 
                if not nodes:
                    # å¦‚æœæ‰¾ä¸åˆ° itemï¼Œå°è¯•æ‰¾ Atom æ ¼å¼çš„ entry
                    nodes = root.findall('.//{http://www.w3.org/2005/Atom}entry')

                for item in nodes[:8]: # é™åˆ¶æ•°é‡ï¼Œé˜²æ­¢ Token æº¢å‡º
                    # === æ ‡é¢˜æŠ“å– ===
                    title = "æ— æ ‡é¢˜"
                    title_node = item.find('title')
                    if title_node is None:
                        # å°è¯•å¸¦å‘½åç©ºé—´çš„ XML æŸ¥æ‰¾
                        title_node = item.find('{http://purl.org/rss/1.0/}title')
                    if title_node is not None:
                        title = title_node.text

                    # === é“¾æ¥æŠ“å– ===
                    link = ""
                    link_node = item.find('link')
                    if link_node is None:
                        link_node = item.find('{http://purl.org/rss/1.0/}link')
                    
                    if link_node is not None:
                        link = link_node.text
                        # å…¼å®¹ Atom: <link href="..." />
                        if not link and link_node.attrib.get('href'):
                            link = link_node.attrib.get('href')

                    # === æè¿°æŠ“å– ===
                    desc = ""
                    desc_node = item.find('description')
                    if desc_node is None:
                        desc_node = item.find('{http://purl.org/rss/1.0/}description')
                    
                    if desc_node is not None and desc_node.text:
                        desc = desc_node.text
                    
                    # æ¸…æ´— HTML æ ‡ç­¾
                    desc = re.sub(r'<[^>]+>', '', desc)
                    
                    if link:
                        items.append({
                            "source": source_name,
                            "title": title,
                            "summary": desc[:200],
                            "url": link
                        })
                print(f"âœ… [{source_name}] è·å–åˆ° {len(items)} æ¡")
            except Exception as e:
                print(f"âŒ [{source_name}] XML è§£æå¤±è´¥: {e}")
        else:
            print(f"âŒ [{source_name}] è¯·æ±‚å¤±è´¥: {response.status_code}")
    except Exception as e:
        print(f"âŒ [{source_name}] ç½‘ç»œé”™è¯¯: {e}")
    
    return items

def fetch_readhub():
    """
    ReadHub ä¸“ç”¨æŠ“å–
    """
    print(f"ğŸ”„ [ReadHub] æ­£åœ¨è¿æ¥...")
    url = "https://api.readhub.cn/topic?pageSize=10"
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Origin": "https://readhub.cn",
        "Referer": "https://readhub.cn/"
    }
    items = []
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        if resp.status_code == 200:
            data = resp.json().get('data', [])
            for d in data:
                items.append({
                    "source": "ReadHub",
                    "title": d.get('title'),
                    "summary": d.get('summary', '')[:200],
                    "url": f"https://readhub.cn/topic/{d.get('id')}"
                })
            print(f"âœ… [ReadHub] è·å–åˆ° {len(items)} æ¡")
        else:
            print(f"âŒ [ReadHub] çŠ¶æ€ç : {resp.status_code}")
    except Exception as e:
        print(f"âŒ [ReadHub] å¤±è´¥: {e}")
    return items

def get_all_news():
    """
    èšåˆæ‰€æœ‰æ•°æ®æº
    """
    all_news = []
    all_news.extend(fetch_readhub())
    all_news.extend(fetch_rss_data("36Kr", "https://36kr.com/feed"))
    all_news.extend(fetch_rss_data("Solidot", "https://www.solidot.org/index.rss"))
    all_news.extend(fetch_rss_data("InfoQ", "https://www.infoq.cn/feed"))
    return all_news

def process_news_with_ai(news_list):
    """
    AI ç­›é€‰ä¸ç‚¹è¯„
    """
    if len(news_list) > 40:
        print(f"âœ‚ï¸ æ–°é—»å¤ªå¤š({len(news_list)}æ¡)ï¼Œæˆªå–å‰ 40 æ¡å–‚ç»™ AI...")
        news_list = news_list[:40]

    print(f"ğŸ§  AI (äº§å“ä¸“å®¶ & ä½“éªŒè®¾è®¡å¸ˆ) æ­£åœ¨é˜…è¯» {len(news_list)} æ¡æ–°é—»...")
    
    raw_text = json.dumps(news_list, ensure_ascii=False)
    
    # === ä¿®å¤ç‚¹ï¼šæ˜ç¡®è¦æ±‚ JSON å¯¹è±¡æ ¼å¼ ===
    system_prompt = """
    ä½ æ˜¯ä¸€ä½æ‹¥æœ‰åŒé‡è§†è§’çš„ä¸“å®¶ï¼šæ—¢æ˜¯ã€èµ„æ·±æ•°æ®äº§å“ä¸“å®¶ã€‘ï¼Œåˆæ˜¯ã€æ•°æ®äº§å“ä½“éªŒè®¾è®¡å¸ˆã€‘ã€‚
    ä½ çš„ä»»åŠ¡æ˜¯ä¸ºåŒè¡Œä¸šè€…ç­›é€‰å‡º 4-6 æ¡æœ€æœ‰ä»·å€¼çš„èµ„è®¯ï¼Œä¸éœ€è¦è¾“å‡ºç¡¬ä»¶ç›¸å…³çš„è®¯æ¯ã€‚
    
    ã€ç­›é€‰ä¼˜å…ˆçº§ã€‘ï¼š
    1. æ ¸å¿ƒå…³æ³¨ï¼šAI Agent äº¤äº’èŒƒå¼ã€æ•°æ®å¯è§†åŒ–åˆ›æ–°ã€BI å·¥å…·çš„æ–°ä½“éªŒè®¾è®¡ã€‚
    2. æ ¸å¿ƒå…³æ³¨ï¼šæ•°æ®æ¶æ„å˜é©ã€å¤§æ¨¡å‹è½åœ°ï¼ˆRAG/å‘é‡åº“ï¼‰çš„æŠ€æœ¯çªç ´å’Œå¤§æ¨¡å‹ç‰ˆæœ¬å‘å¸ƒç›¸å…³çš„ä¿¡æ¯ã€‚
    3. é‡è¦å…³æ³¨ï¼šä¸»æµç§‘æŠ€å¤§å‚ï¼ˆOpenAI/Google/Figmaç­‰ï¼‰å¯¹æ•°æ®äº§å“çš„è®¾è®¡è°ƒæ•´ã€‚
    
    ã€è¾“å‡ºé“å¾‹ã€‘ï¼š
    1. æ ‡é¢˜ï¼šç®€ç»ƒã€ä¸“ä¸šã€‚
    2. ç‚¹è¯„ï¼ˆCommentï¼‰ï¼š
       - å¿…é¡»ç»“åˆâ€œå•†ä¸šä»·å€¼â€æˆ–â€œç”¨æˆ·ä½“éªŒâ€è¿›è¡Œæ·±åº¦æ´å¯Ÿã€‚
       - ä¸¥ç¦å‡ºç°â€œä½œä¸ºè®¾è®¡å¸ˆâ€ã€â€œç¬”è€…è®¤ä¸ºâ€ç­‰èº«ä»½æŒ‡ä»£è¯ã€‚
       - ç›´æ¥è¾“å‡ºè§‚ç‚¹ã€‚
    3. **æ ¼å¼è¦æ±‚**ï¼šå¿…é¡»è¿”å›ä¸€ä¸ªåŒ…å« `news` å­—æ®µçš„ JSON å¯¹è±¡ã€‚

    JSON ç¤ºä¾‹ï¼š
    {
        "news": [
            {
                "title": "é‡å†™åçš„æ ‡é¢˜",
                "source": "æ¥æº",
                "comment": "ç›´æ¥çš„çŠ€åˆ©ç‚¹è¯„",
                "url": "é“¾æ¥"
            }
        ]
    }
    """

    try:
        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"è¯·ç­›é€‰å¹¶åˆ†æï¼š{raw_text}"}
            ],
            response_format={ "type": "json_object" }, 
            temperature=0.3
        )
        content = response.choices[0].message.content
        
        # æ¸…æ´—
        if content.startswith("```"):
            content = re.sub(r"^```json\s*|\s*```$", "", content, flags=re.MULTILINE)
            
        result = json.loads(content)
        
        # === ä¿®å¤ç‚¹ï¼šå…¼å®¹è§£æé€»è¾‘ ===
        # ç°åœ¨çš„ Prompt ä¼šè¿”å› {"news": [...]}, æ‰€ä»¥æˆ‘ä»¬ä¼˜å…ˆå– result["news"]
        if isinstance(result, dict):
            if "news" in result and isinstance(result["news"], list):
                return result["news"]
            # å…œåº•ï¼šå¦‚æœ AI è¿˜æ˜¯è‡ªä½œèªæ˜åªè¿”å›äº† list çš„ dict åŒ…è£…
            for k, v in result.items():
                if isinstance(v, list): return v
        
        return []
        
    except Exception as e:
        print(f"âŒ AI æ€è€ƒå¤±è´¥: {e}")
        # è°ƒè¯•æ‰“å°ï¼Œæ–¹ä¾¿çœ‹ AI åˆ°åº•å›äº†ä»€ä¹ˆ
        print(f"AI åŸå§‹å†…å®¹: {content[:200] if 'content' in locals() else 'ç©º'}")
        return []

def send_wecom(news_list):
    if not WEBHOOK_URL: return
    if not news_list: return

    today = datetime.now().strftime("%Y-%m-%d")
    content_lines = [f"### ğŸš€ AI æ•°æ®äº§å“æ—¥æŠ¥ ({today})"]
    
    for idx, news in enumerate(news_list, 1):
        title = news.get('title', 'æ— æ ‡é¢˜')
        url = news.get('url', '#')
        comment = news.get('comment', 'æ— ç‚¹è¯„')
        src = news.get('source', 'ç²¾é€‰')
        
        content_lines.append(f"**{idx}. {title}**")
        content_lines.append(f"_{src}_  [æŸ¥çœ‹åŸæ–‡]({url})")
        content_lines.append(f"> ğŸ’¡ {comment}") 
        content_lines.append("")

    data = {"msgtype": "markdown", "markdown": {"content": "\n".join(content_lines)}}
    
    try:
        requests.post(WEBHOOK_URL, json=data)
        print("âœ… æ¨é€æˆåŠŸ")
    except Exception as e:
        print(f"âŒ æ¨é€å¤±è´¥: {e}")

if __name__ == "__main__":
    all_data = get_all_news()
    
    if not all_data:
        print("âŒ æ‰€æœ‰æºéƒ½æŠ“å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œã€‚")
    else:
        print(f"ğŸ“¦ æ€»å…±è·å–åˆ° {len(all_data)} æ¡å€™é€‰æ–°é—»")
        final_news = process_news_with_ai(all_data)
        if final_news:
            send_wecom(final_news)
        else:
            print("âš ï¸ AI è§‰å¾—ä»Šå¤©æ²¡ä»€ä¹ˆå€¼å¾—çœ‹çš„æ–°é—»ã€‚")
